{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729f2c87-1fc7-40fa-9ac6-d041cdcdbbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a3fec0-ea6f-4f68-ac45-273e70c36761",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../data/raf-db/train\"\n",
    "test_dir = \"../data/raf-db/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90101fb-2616-4e2d-8779-9726b238fb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12271 images belonging to 7 classes.\n",
      "Found 3068 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_gen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_data = test_gen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964c857f-5e23-4daf-82f4-633e14f3205a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(\n",
    "    input_shape=(224,224,3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c51ca2-c1d3-4856-afff-00f8d9b2cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(7, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d0f5c40-7395-4861-addf-518647e02483",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55925b1f-793c-46b5-abb3-3851a5f4fdf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m362s\u001b[0m 922ms/step - accuracy: 0.4655 - loss: 6.3364 - val_accuracy: 0.5385 - val_loss: 5.2487\n",
      "Epoch 2/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 939ms/step - accuracy: 0.5304 - loss: 6.9542 - val_accuracy: 0.5365 - val_loss: 6.5705\n",
      "Epoch 3/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 925ms/step - accuracy: 0.5540 - loss: 7.6107 - val_accuracy: 0.5968 - val_loss: 6.2597\n",
      "Epoch 4/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m402s\u001b[0m 1s/step - accuracy: 0.5610 - loss: 8.0145 - val_accuracy: 0.5949 - val_loss: 6.6810\n",
      "Epoch 5/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m514s\u001b[0m 1s/step - accuracy: 0.5700 - loss: 8.4479 - val_accuracy: 0.5596 - val_loss: 7.4973\n",
      "Epoch 6/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 1s/step - accuracy: 0.5805 - loss: 8.8136 - val_accuracy: 0.5883 - val_loss: 7.1810\n",
      "Epoch 7/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 1s/step - accuracy: 0.5821 - loss: 8.8765 - val_accuracy: 0.5844 - val_loss: 8.5421\n",
      "Epoch 8/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 1s/step - accuracy: 0.5942 - loss: 9.2439 - val_accuracy: 0.5486 - val_loss: 10.1264\n",
      "Epoch 9/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m559s\u001b[0m 1s/step - accuracy: 0.5926 - loss: 9.6025 - val_accuracy: 0.5945 - val_loss: 9.3662\n",
      "Epoch 10/10\n",
      "\u001b[1m384/384\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 1s/step - accuracy: 0.6014 - loss: 9.5662 - val_accuracy: 0.5786 - val_loss: 10.2531\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=test_data,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44525a9c-cbb4-4e0e-8b04-6d4f7bb301c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"../models/emotion_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcc67d-754d-4468-a3f7-92e61a103479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
